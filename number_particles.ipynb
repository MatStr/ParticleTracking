{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a very simple implementation of a model that counts the number of particles that are created in a scattering event. \n",
    "## The data comes from the LHC where beams of protons and antiprotons are being collided. After each collision event a detector built of several layers is used to track the created particles. The model takes all hits of particles in the detector - in a (x,y,z)-coordinate system - and outputs the number of created particles: a integer number between 0 and 15000, which was my threshold for the maximal number of particles per event. It is a classification problem with a huge number of classes. It might be sensible to make some step-size of 20, or maybe even 100, to reduce the number of classes. This can be decided at a later point. (This project was a part of my work for the Kaggle competition \"TrackML Particle Tracking Challenge\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trackml.dataset import load_event\n",
    "from keras import models, layers\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load one event to get some insight in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_event('train_100_events/event000001000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the detector data into a Euclidean coordinate system, a discretized box of size 100 x 100 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to transform our data in to a det_shape = (x,y,z) form... \n",
    "#det_shape = (1000,1000,1000)\n",
    "#x_max = ymax = 1025 \n",
    "#z_max = 3000\n",
    "def trans_input(hits_input = hits, det_shape = (100,100,100), x_max = 1025,ymax = 1025, z_max = 3000):\n",
    "    hits_input[[\"x\",\"y\"]] = (hits[[\"x\",\"y\"]] + x_max )/(2*x_max) * (det_shape[0]-1)\n",
    "    hits_input[[\"z\"]] = (hits[[\"z\"]] + z_max )/(2*z_max) * (det_shape[2]-1)\n",
    "    hits_input[[\"x\",\"y\",\"z\"]] = hits_input[[\"x\",\"y\",\"z\"]].astype(int)\n",
    "    detector_hits = np.zeros(det_shape)\n",
    "    for each_hit in range( len(hits) ):\n",
    "        xx,yy,zz = hits_input.loc[each_hit][[\"x\",\"y\",\"z\"]]\n",
    "        detector_hits[ xx,yy,zz ] = detector_hits[ xx,yy,zz ] + 1;\n",
    "    return detector_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hits = trans_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_hits.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the first 100 events. The data is quite big, so let us start with 100 to make some tests and load the rest later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ev(i):\n",
    "    if i < 10:\n",
    "        hits, _, _, truth = load_event('train_100_events/event00000100'+str(i))\n",
    "    else:\n",
    "        hits, _, _, truth = load_event('train_100_events/event0000010'+str(i))\n",
    "    return hits, truth\n",
    "\n",
    "max_particles = 15000\n",
    "x_train = new_hits\n",
    "y_train = np.zeros((100,max_particles))\n",
    "y_train[0,len( truth.particle_id.unique() )] = 1\n",
    "\n",
    "for i in range(1,100):\n",
    "    hits, truth = load_ev(i)\n",
    "    if i == 1: \n",
    "        x_train = np.vstack(([x_train], [trans_input(hits)]) )\n",
    "        y_train[i,len( truth.particle_id.unique() )] = 1\n",
    "    else:\n",
    "        x_train = np.vstack((x_train, [trans_input(hits)] )) \n",
    "        y_train[i,len( truth.particle_id.unique() )] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape,x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let me define a model for classification: It is inspired from image classification, so I have some convolution layers (3D in this case) and one fully connected layer in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try some model, set the number of maximal particles to 15000 (empirical value)\n",
    "def counting_model(det_shape, max_particles=15000, optimizer='Nadam'):\n",
    "    \"\"\"\n",
    "    A very simple convolution model that predicts how many particles are in each event.\n",
    "    Output is discrete num_particles class, set up as classification.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=det_shape)\n",
    "    # Reshape to add a channel dimension to the input\n",
    "    hidden = layers.Reshape((1,) + det_shape)(inputs)\n",
    "    # Convolutions and downsampling\n",
    "    hidden = layers.Conv3D(8, (3, 3,3), padding='same', activation='relu')(hidden)\n",
    "    hidden = layers.MaxPooling3D(pool_size=(2, 2, 2), padding='same')(hidden)\n",
    "    hidden = layers.Conv3D(12, (3, 3, 3), padding='same', activation='relu')(hidden)\n",
    "    hidden = layers.MaxPooling3D(pool_size=(2, 2, 2), padding='same')(hidden)\n",
    "    hidden = layers.Conv3D(16, (3, 3, 3), padding='same', activation='relu')(hidden)\n",
    "    hidden = layers.MaxPooling3D(pool_size=(2, 2, 2), padding='same')(hidden)\n",
    "    hidden = layers.Conv3D(20, (3, 3, 3), padding='same', activation='relu')(hidden)\n",
    "    hidden = layers.MaxPooling3D(pool_size=(2, 2, 2), padding='same')(hidden)\n",
    "    hidden = layers.Conv3D(24, (3, 3, 3), padding='same', activation='relu')(hidden)\n",
    "    # Fully connected and softmax\n",
    "    hidden = layers.Flatten()(hidden)\n",
    "    outputs = layers.Dense(max_particles, activation='softmax')(hidden)\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = counting_model(det_shape= (100,100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model (Here only with the first 100 events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, steps_per_epoch=5, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this small set of events the model start to overfit very fast. Let me see how valuable the model is on new data before I continue to train it on the large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(i):\n",
    "    if i < 10:\n",
    "        hits, _, _, truth = load_event('train_200_events/event00000110'+str(i))\n",
    "    else:\n",
    "        hits, _, _, truth = load_event('train_200_events/event0000011'+str(i))\n",
    "    return hits, truth\n",
    "\n",
    "max_particles = 15000\n",
    "hits, _, _, truth = load_event('train_200_events/event000001100')\n",
    "x_test = trans_input(hits)\n",
    "y_test = np.zeros((100,max_particles))\n",
    "y_test[0,len( truth.particle_id.unique() )] = 1\n",
    "\n",
    "for i in range(1,100):\n",
    "    hits, truth = load_test(i)\n",
    "    if i == 1: \n",
    "        x_test = np.vstack(([x_test], [trans_input(hits)]) )\n",
    "        y_test[i,len( truth.particle_id.unique() )] = 1\n",
    "    else:\n",
    "        x_test = np.vstack((x_test, [trans_input(hits)] )) \n",
    "        y_test[i,len( truth.particle_id.unique() )] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
